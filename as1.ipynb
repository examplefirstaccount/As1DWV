{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Web Scraping Highest-Grossing Films from Wikipedia\n",
    "This notebook scrapes Wikipedia's **List of highest-grossing films**, extracting key details like title, release year, director(s), box office revenue, and country of origin.\n",
    "\n",
    "### Supported Wikipedia Tables:\n",
    "I target **four different tables** on the Wikipedia page, each with a unique structure:\n",
    "1. **Highest-grossing films** (Default)\n",
    "2. **Highest-grossing films adjusted for inflation**\n",
    "3. **Highest-grossing films by year of release**\n",
    "4. **Timeline of the highest-grossing film record**\n",
    "\n",
    "By default, I parse only the first table, but the code can be configured to scrape all four tables."
   ],
   "id": "8fa72ac5dd983416"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-24T17:30:21.720049Z",
     "start_time": "2025-02-24T17:30:21.147498Z"
    }
   },
   "source": [
    "# Imports\n",
    "import asyncio\n",
    "import re\n",
    "\n",
    "import httpx\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T17:30:21.750907Z",
     "start_time": "2025-02-24T17:30:21.727079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Constants for parsing\n",
    "WIKI_BASE_URL = 'https://en.wikipedia.org'\n",
    "WIKI_TARGET_PAGE = 'https://en.wikipedia.org/wiki/List_of_highest-grossing_films'\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0'}"
   ],
   "id": "17c32c3504345dca",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fetching the Wikipedia Page\n",
    "I used `requests` (sync) and `httpx` (async) to fetch Wikipedia data.\n",
    "- **`fetch_url(url)`** → Synchronous HTTP request.\n",
    "- **`fetch_url_async(async_client, url)`** → Async request for parallel scraping.\n"
   ],
   "id": "7ed19c0e03d2d65a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T17:30:21.782901Z",
     "start_time": "2025-02-24T17:30:21.755911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_url(url):\n",
    "    \"\"\"Fetches a webpage synchronously using requests.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()  # Handle HTTP errors (4xx/5xx)\n",
    "        return response.text\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f'Error fetching {url}: {e}')\n",
    "        return None\n",
    "\n",
    "\n",
    "async def fetch_url_async(async_client, url):\n",
    "    \"\"\"Fetches a webpage asynchronously using httpx or another async client.\"\"\"\n",
    "    try:\n",
    "        response = await async_client.get(url)\n",
    "        response.raise_for_status()  # Handle HTTP errors (4xx/5xx)\n",
    "        return response.text\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        print(f'Error fetching {url}: {e}')\n",
    "        return None"
   ],
   "id": "9141e689e6ab902d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extracting Film Links\n",
    "The function `get_film_links(parse_all_links=False)` extracts links from the Wikipedia tables.\n",
    "- Default: Parses only **Highest-grossing films**.\n",
    "- If `parse_all_links=True`, it extracts from **all four tables**.\n"
   ],
   "id": "5e76cbea4910603d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T17:30:21.814150Z",
     "start_time": "2025-02-24T17:30:21.786900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_film_links(parse_all_links: bool = False) -> set[str]:\n",
    "    \"\"\"\n",
    "    Extracts Wikipedia film page links from the 'List of highest-grossing films' page.\n",
    "    \n",
    "    Args:\n",
    "        parse_all_links (bool): \n",
    "            - False (default) → Parses only the first table (highest-grossing films).\n",
    "            - True → Parses all four tables.\n",
    "    \n",
    "    Returns:\n",
    "        set[str]: A set of unique Wikipedia URLs for films.\n",
    "    \"\"\"\n",
    "    \n",
    "    page_content = fetch_url(WIKI_TARGET_PAGE)\n",
    "    \n",
    "    if not page_content:\n",
    "        return set()\n",
    "    \n",
    "    soup = BeautifulSoup(page_content, 'lxml')\n",
    "\n",
    "    # Select tables based on the 'parse_all_links' flag\n",
    "    if parse_all_links:\n",
    "        tables = soup.select('table.wikitable.plainrowheaders')  # Get all tables\n",
    "    else:\n",
    "        tables = [soup.select_one('table.wikitable.plainrowheaders')]  # Get only the first table\n",
    "    \n",
    "    film_links = set()\n",
    "\n",
    "    # Loop through each table and extract film links\n",
    "    for i, table in enumerate(tables):\n",
    "        for row in table.select('tbody tr'):\n",
    "\n",
    "            # Handling different table structures:\n",
    "            if i < 2:  # First two tables: Film links are in <th scope=\"row\"><i><a href=\"...\">\n",
    "                th = row.find('th', scope='row')\n",
    "                link = th.find('i').find('a') if th and th.find('i') else None\n",
    "            else:  # Last two tables: Film links are in <td><i><a href=\"...\">\n",
    "                td = row.find('td')\n",
    "                link = td.find('i').find('a') if td and td.find('i') else None\n",
    "\n",
    "            if link and 'href' in link.attrs:\n",
    "                film_links.add(urljoin(WIKI_BASE_URL, link['href']))\n",
    "    \n",
    "    return film_links"
   ],
   "id": "4a7e0aeae6551a05",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Asynchronous Film Data Extraction\n",
    "To speed up scraping, I fetch film pages asynchronously using `httpx.AsyncClient()`.\n",
    "\n",
    "Each film page is parsed using `parse_film_page()`, which extracts relevant details from the `infobox`.\n",
    "\n",
    "## Cleaning and Structuring Data\n",
    "To ensure clean data storage, I perform several transformations:\n",
    "- **Extract only the release year** from messy date formats.\n",
    "- **Remove references (`[1]`, `[2]`)** from text fields.\n",
    "- **Normalize country and director names** into a readable format.\n"
   ],
   "id": "bf66ff62842a8aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T17:30:21.845676Z",
     "start_time": "2025-02-24T17:30:21.819619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "async def parse_film_page(async_client, url):\n",
    "    \"\"\"\n",
    "    Asynchronously fetches and parses a Wikipedia film page to extract key details.\n",
    "\n",
    "    Args:\n",
    "        async_client (httpx.AsyncClient): The async HTTP client for fetching the page.\n",
    "        url (str): The Wikipedia URL of the film.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: A dictionary containing film details (title, release year, director, box office, country, url)\n",
    "                      or None if the page could not be parsed.\n",
    "    \"\"\"\n",
    "    \n",
    "    page_content = await fetch_url_async(async_client, url)\n",
    "    \n",
    "    if not page_content:\n",
    "        return None\n",
    "\n",
    "    # Locate the film's infobox (table containing key details)\n",
    "    soup = BeautifulSoup(page_content, 'lxml')\n",
    "    info_box = soup.select_one('table.infobox.vevent tbody')\n",
    "    \n",
    "    if not info_box:\n",
    "        return None\n",
    "\n",
    "    # Extracts text from an info box row given its label.\n",
    "    def get_info(label):\n",
    "        row = info_box.find('th', string=label)\n",
    "        if row and row.find_next_sibling('td'):\n",
    "            return row.find_next_sibling('td').get_text(', ', strip=True)\n",
    "        return None\n",
    "\n",
    "    # Extracts the four-digit release year from a date string.\n",
    "    def extract_year(date_text):\n",
    "        match = re.search(r'\\b(\\d{4})\\b', date_text)\n",
    "        return int(match.group(1)) if match else None\n",
    "    \n",
    "    # Cleans and formats a string containing multiple values (e.g., directors, countries).\n",
    "    def clean_multivalued(value):\n",
    "        value = re.sub(r'\\[.*?\\]', '', value)  # Removes reference numbers (e.g., [1], [2])\n",
    "        value = re.sub(r'\\(.*?\\)', '', value)  # Removes information in parentheses\n",
    "        \n",
    "        # Splits values by commas and trims whitespace\n",
    "        value_list = [val for val in value.split(', ') if val not in ('', ' ')]\n",
    "        value = ', '.join(value_list).strip(', ')\n",
    "        return value\n",
    "    \n",
    "    # Cleans and formats the box office revenue.\n",
    "    def clean_box_office(value):\n",
    "        value = re.sub(r'\\[.*?\\]', '', value)  # Removes reference numbers\n",
    "        return value.replace('\\xa0', ' ').replace(',', '').strip(', ')\n",
    "\n",
    "    # Extract key details from the infobox\n",
    "    title = info_box.find('tr').text\n",
    "    release_year = get_info('Release date') or get_info(\"Release dates\") or get_info('Released')\n",
    "    director = get_info('Directed by')\n",
    "    box_office = get_info('Box office').replace('US', '')\n",
    "    country = get_info('Country') or get_info('Countries')\n",
    "\n",
    "    # Store the extracted details in a dictionary\n",
    "    film_data = {\n",
    "        'title': title,\n",
    "        'release_year': extract_year(release_year),\n",
    "        'director': clean_multivalued(director) if director else None,\n",
    "        'box_office': clean_box_office(box_office) if box_office else None,\n",
    "        'country': clean_multivalued(country) if country else None,\n",
    "        'url': url\n",
    "    }\n",
    "\n",
    "    # Return the film data only if a title was successfully extracted\n",
    "    return film_data if film_data.get('title') else None"
   ],
   "id": "eed48124396c3688",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T17:30:21.861057Z",
     "start_time": "2025-02-24T17:30:21.849067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "async def parse_film_data(film_links):\n",
    "    \"\"\"\n",
    "    Asynchronously fetches and parses multiple film pages.\n",
    "    \n",
    "    Args:\n",
    "        film_links (list[str]): A list of Wikipedia film URLs.\n",
    "    \n",
    "    Returns:\n",
    "        list[dict]: A list of parsed film data dictionaries.\n",
    "    \"\"\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        tasks = [parse_film_page(client, url) for url in film_links]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        films = [film for film in results if film]\n",
    "    \n",
    "    return films"
   ],
   "id": "9c5f15534918ad5c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T17:30:24.079351Z",
     "start_time": "2025-02-24T17:30:21.866153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get film urls by parsing target wiki page\n",
    "links = list(get_film_links())"
   ],
   "id": "82954dc9f3dc762f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Running Async Parsing in Jupyter\n",
    "Since Jupyter runs an event loop, `asyncio.run()` **cannot be used directly**.\n",
    "\n",
    "`await parse_film_data(links)` **works fine in IPython environments (Jupyter, Colab, Kaggle, DataSpell).**\n",
    "\n",
    "If running in standard Jupyter, use:\n",
    "```python\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "```"
   ],
   "id": "899b39a847bf988d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T17:31:05.545755Z",
     "start_time": "2025-02-24T17:30:24.083546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parse all data from film urls\n",
    "data = await parse_film_data(links)\n",
    "data.sort(key=lambda film: film.get('box_office', 0), reverse=True)"
   ],
   "id": "b22e6a887390ccb2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Saving Data to Database\n",
    "The extracted data is stored in an SQLite database. I ensure:\n",
    "- Unique constraints to prevent duplicate entries.\n",
    "- Proper data types (`INT` for year, `TEXT` for other fields).\n"
   ],
   "id": "b9bf4e7a778acfae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T17:31:06.184958Z",
     "start_time": "2025-02-24T17:31:05.550289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "from sqlalchemy import create_engine, Column, Integer, String\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base"
   ],
   "id": "a4d39d5fd1297844",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T17:31:06.200273Z",
     "start_time": "2025-02-24T17:31:06.188734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Database file path (SQLite database)\n",
    "DB_URL = 'sqlite:///grossing_films.db'"
   ],
   "id": "55a96bf81dc209e0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T17:31:06.293471Z",
     "start_time": "2025-02-24T17:31:06.204312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a database engine\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "# Base class for ORM models\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class Film(Base):\n",
    "    \"\"\"\n",
    "    SQLAlchemy ORM model representing a film record.\n",
    "    \n",
    "    Attributes:\n",
    "        id (int): Primary key, auto-incremented.\n",
    "        title (str): The film's title (required).\n",
    "        release_year (int): Year of release.\n",
    "        director (str): Name(s) of the director(s).\n",
    "        box_office (str): Box office revenue.\n",
    "        country (str): Country of origin.\n",
    "        url (str): Wikipedia URL for reference.\n",
    "    \"\"\"\n",
    "    __tablename__ = 'films'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    title = Column(String, nullable=False)\n",
    "    release_year = Column(Integer)\n",
    "    director = Column(String)\n",
    "    box_office = Column(String)\n",
    "    country = Column(String)\n",
    "    url = Column(String)\n",
    "\n",
    "# Create the database table (if it doesn't already exist)\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "# Create a session for interacting with the database\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Remove existing records to avoid duplicates before inserting new ones\n",
    "session.query(Film).delete()\n",
    "\n",
    "# Add all film records to the session and commit transaction\n",
    "film_rows = [Film(**film) for film in data]\n",
    "session.add_all(film_rows)\n",
    "session.commit()"
   ],
   "id": "66e491517dc2df4c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Saving Data to JSON\n",
    "The extracted data is exported to JSON file for further usage in frontend."
   ],
   "id": "d54b4ba7d8f492f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T17:31:06.324727Z",
     "start_time": "2025-02-24T17:31:06.296816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Save the scraped data as a JSON file\n",
    "with open('grossing_films.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ],
   "id": "929ef205ab51edf",
   "outputs": [],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
